# YOLO12: 주의 집중 객체 감지

## 개요

YOLO12는 기존 YOLO 모델의 CNN 기반 접근 방식에서 벗어나 관심도 중심 아키텍처를 도입한 혁신적인 객체 감지 모델입니다. 실시간 추론 속도를 유지하면서 주의 메커니즘과 전반적인 네트워크 아키텍처의 새로운 방법론적 혁신을 통해 최첨단 물체 감지 정확도를 달성합니다.

## 주요 기능

### 1. 영역 주의 메커니즘
- 대규모 수용 필드를 효율적으로 처리하는 새로운 자체 주의 접근 방식
- 피처 맵을 가로 또는 세로로 동일한 크기의 영역(기본값 4개)으로 분할
- 표준 셀프 어텐션 대비 계산 비용 크게 절감

### 2. 잔여 효율적 계층 집계 네트워크 (R-ELAN)
- 대규모 주의 집중 모델에서 최적화 문제를 해결하는 ELAN 기반 향상된 기능 집계 모듈
- 스케일링이 있는 블록 수준의 잔여 연결 (레이어 스케일링과 유사)
- 병목 현상과 같은 구조를 만드는 재설계된 피처 집계 방식

### 3. 최적화된 주의 집중 아키텍처
- 플래시어텐션을 사용하여 메모리 액세스 오버헤드 최소화
- 위치 인코딩 제거로 더 깔끔하고 빠른 모델 구현
- MLP 비율 조정 (일반적인 4에서 1.2 또는 2로) 
- 스택 블록의 깊이를 줄여 최적화 개선
- 주의 메커니즘에 7x7 분리 가능한 컨볼루션 ('위치 인식기') 추가

### 4. 포괄적인 작업 지원
- 객체 감지
- 인스턴스 분할
- 이미지 분류
- 포즈 추정
- 방향성 객체 감지 (OBB)

### 5. 향상된 효율성
- 이전 모델 대비 더 적은 파라미터로 더 높은 정확도 달성
- 속도와 정확도 간의 균형 개선
- 엣지 디바이스에서 클라우드 인프라까지 다양한 플랫폼 배포 지원

## 지원되는 작업 및 모드

| 모델 유형 | 작업 | 추론 | 유효성 검사 | 교육 | 내보내기 |
|-----------|------|------|-------------|------|----------|
| YOLO12 | 탐지 | ✅ | ✅ | ✅ | ✅ |
| YOLO12-seg | 세분화 | ✅ | ✅ | ✅ | ✅ |
| YOLO12-pose | 포즈 | ✅ | ✅ | ✅ | ✅ |
| YOLO12-cls | 분류 | ✅ | ✅ | ✅ | ✅ |
| YOLO12-obb | OBB | ✅ | ✅ | ✅ | ✅ |

## 성능 지표

### 탐지 성능 (COCO val2017)

| 모델 | 크기 (픽셀) | mAP val 50-95 | 속도 CPU ONNX (ms) | 속도 T4 TensorRT (ms) | 매개변수 (M) | FLOPs (B) |
|-------|-------------|---------------|--------------------|-----------------------|--------------|-----------|
| YOLO12n | 640 | 39.8 | 221.7 | 1.5 | 2.9 | 7.8 |
| YOLO12s | 640 | 47.0 | 336.8 | 2.3 | 11.5 | 28.6 |
| YOLO12m | 640 | 53.0 | 668.7 | 4.7 | 26.1 | 75.4 |
| YOLO12l | 640 | 54.4 | 1059.9 | 8.0 | 46.0 | 142.9 |
| YOLO12x | 640 | 55.4 | 1498.9 | 11.2 | 68.6 | 220.9 |

> 추론 속도는 TensorRT FP16 정밀도를 갖춘 NVIDIA T4 GPU에서 측정되었습니다.

## 사용 예시

### Python 예시

```python
from ultralytics import YOLO

# COCO 사전 훈련된 YOLO12n 모델 로드
model = YOLO("yolo12n.pt")

# COCO8 예시 데이터셋으로 100 에포크 훈련
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

# 'bus.jpg' 이미지에서 추론 실행
results = model("path/to/bus.jpg")
```

### CLI 사용법

```bash
# 모델 훈련
yolo train model=yolo12n.pt data=coco8.yaml epochs=100 imgsz=640

# 추론 실행
yolo predict model=yolo12n.pt source=path/to/bus.jpg
```

## 주요 개선 사항

### 향상된 기능 추출
- **영역 주의**: 대규모 수신 필드를 효율적으로 처리하여 계산 비용 절감
- **최적화된 균형**: 주의 집중과 피드 포워드 네트워크 계산 간의 균형 개선
- **R-ELAN**: 개선된 기능 집계 아키텍처

### 최적화 혁신
- **잔여 연결**: 특히 대규모 모델에서 훈련 안정화
- **개선된 기능 통합**: R-ELAN 내에서 기능 통합 방법 개선
- **플래시어텐션**: 메모리 액세스 오버헤드 감소

### 아키텍처 효율성
- **매개변수 감소**: 정확도 유지/개선과 동시에 더 적은 매개변수 사용
- **간소화된 주의**: 위치 인코딩을 피하는 간소화된 주의 구현
- **최적화된 MLP 비율**: 컴퓨팅 리소스의 효과적 할당

## 하드웨어 요구 사항

### 기본 사용
- 기본적으로 플래시어텐션 불필요
- 일반적인 GPU 환경에서 실행 가능

### 플래시어텐션 사용 (선택사항)
다음 NVIDIA GPU 중 하나 필요:
- **Turing GPU**: T4, Quadro RTX 시리즈
- **Ampere GPU**: RTX30 시리즈, A30/40/100
- **Ada Lovelace GPU**: RTX40 시리즈
- **Hopper GPU**: H100/H200

## 인용

연구에 YOLO12를 사용하는 경우 다음을 인용해 주세요:

```bibtex
@article{tian2025yolov12,
  title={YOLOv12: Attention-Centric Real-Time Object Detectors},
  author={Tian, Yunjie and Ye, Qixiang and Doermann, David},
  journal={arXiv preprint arXiv:2502.12524},
  year={2025}
}

@software{yolo12,
  author = {Tian, Yunjie and Ye, Qixiang and Doermann, David},
  title = {YOLOv12: Attention-Centric Real-Time Object Detectors},
  year = {2025},
  url = {https://github.com/sunsmarterjie/yolov12},
  license = {AGPL-3.0}
}
```

## 자주 묻는 질문 (FAQ)

### Q1: YOLO12는 어떻게 높은 정확도를 유지하면서 실시간 물체 감지를 달성할 수 있나요?
A: YOLO12는 영역 주의 메커니즘, R-ELAN 아키텍처, 최적화된 어텐션 구조를 통해 계산 비용을 절감하면서 정확도를 향상시킵니다.

### Q2: YOLO12는 어떤 컴퓨터 비전 작업을 지원하나요?
A: 물체 감지, 인스턴스 분할, 이미지 분류, 포즈 추정, 방향성 물체 감지(OBB) 등을 지원합니다.

### Q3: 다른 YOLO 모델과 비교했을 때 YOLO12의 성능은 어떤가요?
A: YOLO12는 모든 모델 스케일에서 YOLOv10 및 YOLO11 대비 상당한 정확도 향상을 보여주며, 속도에서는 약간의 트레이드오프가 있습니다.

### Q4: YOLO12 사용을 위한 자세한 문서는 어디에서 찾을 수 있나요?
A: Ultralytics 공식 문서의 예측, 훈련, 검증, 내보내기 섹션과 각 작업별 전용 페이지를 참조하세요.

---
