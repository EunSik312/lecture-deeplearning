\*\*CNN (Convolutional Neural Network)\*\*
| Term                                 | Description                                                                                         |
| ------------------------------------ | --------------------------------------------------------------------------------------------------- |
| **Convolution**                      | Sliding a filter (kernel) over the input to extract features like edges, textures, etc.             |
| **Kernel / Filter**                  | A small matrix used to perform convolution operation; detects patterns in the input.                |
| **Stride**                           | The step size by which the kernel moves across the input.                                           |
| **Padding**                          | Adding extra borders (usually zeros) around the input to preserve spatial dimensions.               |
| **ReLU (Rectified Linear Unit)**     | Activation function that replaces negative values with zero to introduce non-linearity.             |
| **Pooling**                          | Downsampling operation (e.g., max pooling, average pooling) to reduce spatial size.                 |
| **Max Pooling**                      | Retains the maximum value in each patch of the feature map.                                         |
| **Average Pooling**                  | Computes the average of values in each patch.                                                       |
| **Feature Map**                      | The output of a convolutional layer, representing learned features.                                 |
| **Flatten**                          | Converts multi-dimensional feature maps into a 1D vector before the fully connected layers.         |
| **Fully Connected Layer (FC Layer)** | Each neuron is connected to all neurons in the previous layer; used for classification.             |
| **Dropout**                          | Regularization technique that randomly disables neurons during training to prevent overfitting.     |
| **Epoch**                            | One complete pass through the entire training dataset.                                              |
| **Batch Size**                       | Number of training samples processed before the model's internal parameters are updated.            |
| **Backpropagation**                  | Algorithm for updating weights using gradients computed from loss.                                  |
| **Gradient Descent**                 | Optimization algorithm to minimize loss by updating weights in the direction of negative gradients. |
| **Overfitting**                      | When the model performs well on training data but poorly on unseen data.                            |
| **Underfitting**                     | When the model fails to capture patterns in the training data.                                      |
| **Regularization**                   | Techniques (e.g., L2, dropout) used to reduce overfitting.                                          |
| **Activation Map**                   | Another term for a feature map, emphasizing the effect of activation functions.                     |


