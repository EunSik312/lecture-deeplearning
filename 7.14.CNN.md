0\*\*CNN (Convolutional Neural Network)\*\*
https://docs.google.com/document/d/1Np73nrbqW8hE7hS-2t1sqhM_OFPvciHb35YqMK60ceo/edit?tab=t.0

#### CNN - 사람이 무의식적으로 수행하는 '보는'행위의 놀라운 과정을 이해할 수 있는 도구
### 핵심: 신경망 연결 구조를 효과적으로 설게한 것, 멀리 있는 픽셀들의 정보도 활용하되, 여러 단계의 정제 과정을 거친 후에 활용함 
| Term                                 | Description                                                                                         |
| ------------------------------------ | --------------------------------------------------------------------------------------------------- |
| **Convolution**                      | Sliding a filter (kernel) over the input to extract features like edges, textures, etc.             |
| **Kernel / Filter**                  | A small matrix used to perform convolution operation; detects patterns in the input.                |
| **Stride**                           | The step size by which the kernel moves across the input.                                           |
| **Padding**                          | Adding extra borders (usually zeros) around the input to preserve spatial dimensions.               |
| **ReLU (Rectified Linear Unit)**     | Activation function that replaces negative values with zero to introduce non-linearity.             |
| **Pooling**                          | Downsampling operation (e.g., max pooling, average pooling) to reduce spatial size.                 |
| **Max Pooling**                      | Retains the maximum value in each patch of the feature map.                                         |
| **Average Pooling**                  | Computes the average of values in each patch.                                                       |
| **Feature Map**                      | The output of a convolutional layer, representing learned features.                                 |
| **Flatten**                          | Converts multi-dimensional feature maps into a 1D vector before the fully connected layers.         |
| **Fully Connected Layer (FC Layer)** | Each neuron is connected to all neurons in the previous layer; used for classification.             |
| **Dropout**                          | Regularization technique that randomly disables neurons during training to prevent overfitting.     |
| **Epoch**                            | One complete pass through the entire training dataset.                                              |
| **Batch Size**                       | Number of training samples processed before the model's internal parameters are updated.            |
| **Backpropagation**                  | Algorithm for updating weights using gradients computed from loss.                                  |
| **Gradient Descent**                 | Optimization algorithm to minimize loss by updating weights in the direction of negative gradients. |
| **Overfitting**                      | When the model performs well on training data but poorly on unseen data.                            |
| **Underfitting**                     | When the model fails to capture patterns in the training data.                                      |
| **Regularization**                   | Techniques (e.g., L2, dropout) used to reduce overfitting.                                          |
| **Activation Map**                   | Another term for a feature map, emphasizing the effect of activation functions.                     |

## 경계 검출 필터 (Edge Detection)

**수직 경계 검출 필터**
```
[-1  0  1]
[-2  0  2]
[-1  0  1]
```
- Sobel 수직 필터
- 세로 선, 수직 경계 강조
- 좌우 밝기 차이 감지

**수평 경계 검출 필터**
```
[-1 -2 -1]
[ 0  0  0]
[ 1  2  1]
```
- Sobel 수평 필터
- 가로 선, 수평 경계 강조
- 상하 밝기 차이 감지

**Prewitt 필터**
```
수직: [-1  0  1]    수평: [-1 -1 -1]
     [-1  0  1]          [ 0  0  0]
     [-1  0  1]          [ 1  1  1]
```
- Sobel과 유사하지만 가중치 다름
- 노이즈에 더 민감

**Laplacian 필터**
```
[ 0 -1  0]
[-1  4 -1]
[ 0 -1  0]
```
- 모든 방향 경계 검출
- 2차 미분 연산자
- 노이즈에 민감

## 블러 필터 (Blur/Smoothing)

**평균 필터 (Average Filter)**
```
[1/9  1/9  1/9]
[1/9  1/9  1/9]
[1/9  1/9  1/9]
```
- 주변 픽셀들의 평균값
- 단순한 노이즈 제거
- 경계도 함께 흐려짐

**가우시안 필터 (Gaussian Filter)**
```
[1/16  2/16  1/16]
[2/16  4/16  2/16]
[1/16  2/16  1/16]
```
- 정규분포 기반 가중치
- 자연스러운 블러 효과
- 노이즈 제거에 효과적

**모션 블러 필터**
```
대각선: [1/3  0   0 ]
       [0   1/3  0 ]
       [0   0   1/3]
```
- 움직임 효과 시뮬레이션
- 방향성 있는 블러

## 샤프닝 필터 (Sharpening)

**기본 샤프닝 필터**
```
[ 0 -1  0]
[-1  5 -1]
[ 0 -1  0]
```
- 경계 강조로 선명도 향상
- 중앙값 증폭, 주변값 감소

**언샤프 마스킹 (Unsharp Masking)**
```
[-1/9 -1/9 -1/9]
[-1/9 17/9 -1/9]
[-1/9 -1/9 -1/9]
```
- 원본 - 블러 + 원본
- 세밀한 디테일 강조
- 사진 편집에서 널리 사용

## 특수 효과 필터

**엠보싱 필터 (Embossing)**
```
[-2 -1  0]
[-1  1  1]
[ 0  1  2]
```
- 3D 입체 효과
- 그림자와 하이라이트 생성
- 예술적 효과

**외곽선 강조 필터**
```
[-1 -1 -1]
[-1  8 -1]
[-1 -1 -1]
```
- 경계만 남기고 내부 제거
- 스케치 효과
- 윤곽선 추출

## 노이즈 제거 필터

**미디언 필터 (Median Filter)**
- 주변 픽셀들의 중간값 선택
- 소금-후추 노이즈 제거에 효과적
- 경계 보존하면서 노이즈 제거

**양방향 필터 (Bilateral Filter)**
- 거리와 픽셀값 차이 모두 고려
- 경계 보존하면서 스무딩
- 고급 노이즈 제거

## CNN에서의 활용

**초기 CNN 연구**
- 수작업으로 설계된 필터들 사용
- Gabor 필터, Sobel 필터 등을 초기 층에 적용
- 특정 패턴 감지를 위한 고정 필터

**현대 CNN**
- 학습 가능한 필터로 대체
- 데이터에서 자동으로 최적 필터 학습
- 하지만 여전히 전통적 필터의 개념을 기반으로 함

**하이브리드 접근**
- 전통적 필터를 초기값으로 사용
- fine-tuning을 통해 최적화
- 해석 가능한 특징 추출



# <혁펜하임의 이지 딥러닝>
## CNN


컨볼루션의 동작 방식: 컨볼루션은 이미지의 일부분을 연결함

일부분을 연결해서 웨이트를 곱하고 바이어스를 더한뒤 액티베이션 함수를 통과시켜 하나의 노드를 생성함

가까이 밀집한 픽셀을 연결함으로 위치 정보가 보존됨, 각 노드는 특정 영역을 담당함

적은 파라미터로 효율적인 학습이 가능함

컨볼루션에 이용되는 웨이트 세트는 행렬로 나타내고 이것을 Kernel or Filter이라고 부름

바이어스는 뉴런의 출력값을 조정해주는 추가 상수 항

컨볼루션 연산의 핵심은 벡터간의 내적과 동일수직

컨볼루션을 거친 이미지는 어떤 특징이 어디에 얼마나 강하게 존재하는지를 보여준다

##이미지 처리 파이프라인 수직 수평 필터
https://claude.ai/public/artifacts/2c09bc56-7cc3-4ea0-b3ca-7678aa107756

<img width="1385" height="746" alt="image" src="https://github.com/user-attachments/assets/b5de3cce-5e30-4cb8-aca2-bf2000902c64" />
이미지 업로드-수직 필터 이용
<img width="1343" height="562" alt="image" src="https://github.com/user-attachments/assets/369eb81c-fcfc-4efb-b03d-5b111ea1981f" />
이미지 업로드 - 수평 필터 이용
<img width="1353" height="567" alt="image" src="https://github.com/user-attachments/assets/5841867b-4128-4eb6-88ea-95764118851b" />
이미지 업로드 - 블러 필터 이용
<img width="1363" height="582" alt="image" src="https://github.com/user-attachments/assets/39cb35a1-3594-4058-8ffb-be4453936c29" />
이미지 업로드 - 샤프닝 필터 이용
<img width="1350" height="566" alt="image" src="https://github.com/user-attachments/assets/1d6ab25f-3392-4e48-8e6e-e9b7fd0999f2" />
이미지 업로드 - 결과
<img width="1350" height="420" alt="image" src="https://github.com/user-attachments/assets/1a7f1f45-7581-4b34-bbfb-fa9c7b4534b1" />




## CNN 파이프라인 시각화 패딩,렐루
https://claude.ai/public/artifacts/df7a5986-dd0a-4a16-af85-ad90959de392

<img width="1414" height="712" alt="image" src="https://github.com/user-attachments/assets/14a2d4ee-9d63-4740-a7fc-bab3774ab9b5" />
<img width="1411" height="714" alt="image" src="https://github.com/user-attachments/assets/4459aec3-e488-40a0-8a83-43527813be03" />
<img width="1402" height="560" alt="image" src="https://github.com/user-attachments/assets/fcc70560-c231-43da-8fd8-15ee45a3a34c" />
<img width="1379" height="705" alt="image" src="https://github.com/user-attachments/assets/0ca4a692-1248-4f9c-a3b2-b2ebe597c303" />
<img width="1410" height="605" alt="image" src="https://github.com/user-attachments/assets/d03d9427-9263-4ca2-abfa-0496e6186db6" />


## 샤프닝 필터
1. **기본 개념**: 샤프닝 필터란 무엇인가
2. **구조와 원리**: 어떻게 작동하는가
3. **다양한 종류**: 기본/강한/부드러운 샤프닝
4. **비교표**: 블러 vs 샤프닝 차이점
5. **실제 사용 예시**: 사진 보정, 컴퓨터 비전
6. **주의사항**: 부작용과 올바른 사용법
7. **신호등 검출**: 실제 프로젝트에서의 활용

---

## ✅ ReLU를 왜 쓰는가?

### ReLU (Rectified Linear Unit) 함수

```python
f(x) = max(0, x)
```

* 입력값이 **0 이하일 때는 0**,
* 입력값이 **0보다 크면 그대로 출력**

---

## 🎯 왜 CNN에서 ReLU로 음수를 제거할까?

### 1. ✅ **비선형성(non-linearity)를 추가하려고**

* CNN의 합성곱(Convolution) 연산은 **선형 연산**입니다.
* 선형 연산만 계속 쌓으면, 아무리 많이 쌓아도 결국 **하나의 선형 함수**밖에 표현 못 해요.
* ReLU 같은 **비선형 함수**를 중간에 넣어야 CNN이 **복잡한 패턴, 경계, 구조**를 표현할 수 있어요.

---

### 2. ✅ **학습 안정성 & 속도 개선**

* ReLU는 **계산이 매우 간단**해서 학습이 빠릅니다.
* 다른 함수들(Sigmoid, Tanh)은 계산이 복잡하고 **기울기 소실(vanishing gradient)** 문제가 잘 생깁니다.
* ReLU는 그걸 피하면서도 **효율적으로 작동**해요.

---

### 3. ✅ **음수 제거 = 중요한 정보만 남긴다**

* CNN의 필터는 특징을 추출하는 역할을 합니다.
* 이때 나오는 음수값은 **해당 특징이 없다거나 약하다는 신호**일 수 있어요.
* ReLU는 그런 약한/불필요한 신호(음수)를 제거해서 **더 강한 특징에 집중**하게 만듭니다.

---

## 📉 음수를 그대로 두면 생길 수 있는 문제

| 문제                    | 설명                               |
| --------------------- | -------------------------------- |
| **출력 신호가 작아짐**        | 음수 + 양수 혼합되면 다음 층으로 전달되는 정보가 약해짐 |
| **활성화 평균값이 0 근처로 수렴** | 전체 신호가 죽어서 학습이 느려짐               |
| **비효율적인 학습**          | 쓸모없는 정보도 계속 전달됨                  |

---

## 📌 예시: ReLU vs ReLU 없음

```python
# Without ReLU
conv_output = -3.4   # 다음 층에도 음수가 전달됨 → 정보 약함

# With ReLU
relu_output = max(0, -3.4) = 0  # 음수 제거 → 신호 OFF
```

→ CNN은 이런 식으로 **필요 없는 신호를 차단**하면서 중요한 특징만 통과시키는 구조로 작동해요.

---

## ✅ 그럼... 음수가 무조건 나쁘다는 뜻인가요?

아니요. ReLU는 "음수 = 나쁜 것"이라기보다는,

> ❗ **학습 효율성과 정보 전달을 위해 일부러 ‘음수 신호를 제거’하는 전략**입니다.

그리고 ReLU에도 단점이 있어서 다음과 같은 변형도 있어요:

| 함수             | 설명                                                 |
| -------------- | -------------------------------------------------- |
| **Leaky ReLU** | 음수일 때도 아주 약한 기울기 유지 (`f(x) = x if x>0 else 0.01x`) |
| **ELU, SELU**  | 음수도 일정 비율로 살려서 더 부드러운 학습 유도                        |

---

## 🔍 요약

| 질문                | 답변                            |
| ----------------- | ----------------------------- |
| **왜 ReLU를 쓰나요?**  | 비선형성 추가, 학습 빠름, 음수 제거로 정보 강조  |
| **왜 음수를 제거하나요?**  | 쓸모없는 신호 제거, 강한 특징에 집중         |
| **음수는 무조건 나쁜가요?** | 그렇진 않지만, 제거하면 학습이 더 효율적일 수 있음 |

---
