## 📊 mAP 그래프

### ✅ 정의

\*\*mAP(mean Average Precision)\*\*는 객체 탐지에서 **예측이 얼마나 정확한지**를 수치로 나타낸다.

* **AP (Average Precision)**: 한 클래스에 대해 precision-recall 곡선의 면적
* **mAP**: 모든 클래스의 AP 평균값

> 예: 3개의 클래스가 있다면
> `mAP = (AP_class1 + AP_class2 + AP_class3) / 3`

---

## 📈 mAP 관련 그래프 종류

| 그래프                         | 설명                                               |
| --------------------------- | ------------------------------------------------ |
| **Precision-Recall Curve**  | 다양한 confidence threshold에서 precision과 recall의 관계 |
| **mAP vs Epoch**            | 훈련 중 mAP 성능이 어떻게 변화하는지 시각화                       |
| **Class-wise AP bar chart** | 각 클래스별 Average Precision 표시                      |
| **IoU vs mAP**              | IoU(교차/합 비율)가 높아질수록 mAP이 어떻게 변하는지 보여줌            |

---

### 🔍 예시: PR Curve (Precision-Recall)

![예시: PR 곡선](https://raw.githubusercontent.com/ultralytics/yolov5/master/docs/images/pr_curve.png)

* 빨간색: precision ↑
* 파란색: recall ↑
  곡선 아래 면적이 클수록 AP가 높고, 결국 mAP도 높음.

---

## 🧪 YOLO 훈련 시 mAP 그래프 확인 방법

Ultralytics YOLO (`v8~v12`)로 모델 훈련 시, 자동으로 성능 그래프됨.

```bash
runs/detect/train/  # 내부에 다음과 같은 그래프 있음:
├── results.png       # Loss, mAP, Precision 등 종합 그래프
├── PR_curve.png      # PR 커브
├── F1_curve.png      # F1-score 곡선
├── confusion_matrix.png  # 혼동 행렬
```

또는 코드에서 직접 확인:

```python
from IPython.display import Image
Image(filename='runs/detect/train/results.png')
```

---

## ✅ 요약

| 항목             | 설명                                |
| -------------- | --------------------------------- |
| **mAP**        | 객체 탐지 정확도의 대표적인 성능 지표             |
| **mAP 그래프 종류** | PR 곡선, epoch 변화 그래프, 클래스별 막대그래프 등 |
| **높을수록 좋은 값**  | 보통 0.5 이상이면 양호, 0.7 이상이면 매우 우수    |
| **자동 생성 위치**   | `runs/detect/train/` 폴더 내 그래프 파일들 |

---


## 📁 mAP 그래프 저장 위치

### ✅ 기본 경로 (Ultralytics 기준)

```bash
runs/detect/train/
```

이 폴더는 훈련이 끝나면 자동으로 생성, 내부에 다양한 그래프와 로그가 포함.

---

### 📄 주요 파일들

| 파일 이름                      | 내용                                           |
| -------------------------- | -------------------------------------------- |
| **`results.png`**          | 훈련 과정에서의 mAP, precision, recall, loss 변화 그래프 |
| **`PR_curve.png`**         | 클래스별 Precision-Recall 곡선 (mAP과 직결)           |
| **`F1_curve.png`**         | F1 점수 변화 (정확도와 재현율의 조화 평균)                   |
| **`confusion_matrix.png`** | 클래스별 정오분류 행렬                                 |
| **`metrics/` 폴더**          | mAP 수치 및 그래프의 원천 데이터 (`.npy` 파일 등)           |

---

### 📍 그래프 예시 보기 (Colab/Python)

```python
from IPython.display import Image

# 종합 결과 그래프 보기
Image(filename='runs/detect/train/results.png')

# PR 커브 보기 (정밀도-재현율)
Image(filename='runs/detect/train/PR_curve.png')
```

---

## 🧪 mAP 수치 직접 보기

```python
results = model.train(data='your_data.yaml', epochs=50)

# 훈련 후 mAP 값 출력
print(results.metrics)  # or results.results_dict
```

또는 터미널 로그 마지막 줄을 보면 아래와 같은 정보 나옴:

```
metrics/precision, recall, mAP50, mAP50-95: 0.89, 0.85, 0.92, 0.65
```

* `mAP50`: IoU ≥ 0.5 기준의 정확도
* `mAP50-95`: IoU 0.5\~0.95 평균 → **정밀 성능지표**

---

## 🔍 결과 폴더 예시 구조

```bash
runs/detect/train/
├── weights/
│   ├── best.pt
│   └── last.pt
├── results.png              ✅ ← mAP 그래프 포함
├── PR_curve.png             ✅ ← 클래스별 PR 곡선
├── confusion_matrix.png     ✅
├── F1_curve.png
├── opt.yaml                 # 훈련 옵션 기록
├── results.csv              # 정량 데이터
└── metrics/
```

---

## ✅ 요약

| 확인 방법             | 설명                           |
| ----------------- | ---------------------------- |
| `results.png`     | mAP, precision, loss 등이 한 눈에 |
| `PR_curve.png`    | 클래스별 정확도/재현율 커브              |
| `results.metrics` | 코드로 직접 수치 보기                 |
| 위치                | `runs/detect/train/` 폴더      |

---
